{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sepeehr/anaconda3/envs/DLC/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from framework import (\n",
    "    SGD, MSE, ReLU, Sigmoid, Sequential,\n",
    "    Conv2d, TransposeConv2d,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ReLU, Sigmoid, MSE, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3, requires_grad=True) - .5\n",
    "y = torch.rand_like(x, requires_grad=True) - .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for func_, func in zip(\n",
    "    [ReLU(), Sigmoid(), Sequential(ReLU(), Sigmoid())],\n",
    "    [nn.ReLU(), nn.Sigmoid(), nn.Sequential(nn.ReLU(), nn.Sigmoid())],\n",
    "):\n",
    "    torch.testing.assert_allclose(func_(x), func(x))\n",
    "    torch.testing.assert_allclose(\n",
    "        func_.backward(grad=torch.ones_like(x)),\n",
    "        torch.autograd.grad(func(x), x, grad_outputs=torch.ones_like(x))[0],\n",
    "    )\n",
    "\n",
    "func_, func = MSE(), nn.MSELoss()\n",
    "torch.testing.assert_allclose(func_(x, y), func(x, y))\n",
    "torch.testing.assert_allclose(\n",
    "    func_.backward(grad=torch.ones_like(x))[0],\n",
    "    torch.autograd.grad(func(x, y), (x, y))[0],\n",
    ")\n",
    "torch.testing.assert_allclose(\n",
    "    func_.backward(grad=torch.ones_like(x))[1],\n",
    "    torch.autograd.grad(func(x, y), (x, y))[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:80] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "in_channels, out_channels = 3, 32\n",
    "settings = dict(\n",
    "    kernel_size=2,\n",
    "    stride=2,\n",
    "    padding=0,\n",
    "    dilation=1,\n",
    "    )\n",
    "\n",
    "# Make sure that the randomly generated parameters work fine\n",
    "x = torch.randn((50, in_channels, 64, 64)).requires_grad_()\n",
    "conv_gen = Conv2d(\n",
    "    in_channels, out_channels, **settings,\n",
    "    params=None,\n",
    "    )\n",
    "y_ = conv_gen(x)\n",
    "_ = conv_gen.backward(grad=torch.ones_like(y_))\n",
    "\n",
    "# Build the moduels and assert the parameters are identical\n",
    "conv = nn.Conv2d(\n",
    "    in_channels, out_channels, **settings\n",
    "    )\n",
    "conv_ = Conv2d(\n",
    "    in_channels, out_channels, **settings,\n",
    "    params=dict(weight=conv.weight.clone(), bias=conv.bias.clone())\n",
    "    )\n",
    "torch.testing.assert_allclose(conv_.param()[0][0], conv.weight)\n",
    "torch.testing.assert_allclose(conv_.param()[1][0], conv.bias)\n",
    "\n",
    "# Check the forward and backward methods\n",
    "x = torch.randn((50, in_channels, 64, 64)).requires_grad_()\n",
    "y, y_ = conv(x), conv_(x)\n",
    "torch.testing.assert_allclose(y_, y)\n",
    "torch.testing.assert_allclose(\n",
    "        conv_.backward(grad=torch.ones_like(y_)),\n",
    "        torch.autograd.grad(conv(x), x, grad_outputs=torch.ones_like(y))[0],\n",
    "    )\n",
    "\n",
    "# Check the parameter gradients\n",
    "gradw, = torch.autograd.grad(conv(x), conv.weight, grad_outputs=torch.ones_like(y))\n",
    "gradb, = torch.autograd.grad(conv(x), conv.bias, grad_outputs=torch.ones_like(y))\n",
    "_, gradw_ = conv_.param()[0]\n",
    "_, gradb_ = conv_.param()[1]\n",
    "torch.testing.assert_allclose(gradw_, gradw)\n",
    "torch.testing.assert_allclose(gradb_, gradb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test TransposeConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels, out_channels = 3, 32\n",
    "settings = dict(\n",
    "    kernel_size=2,\n",
    "    stride=4,\n",
    "    padding=3,\n",
    "    dilation=1,\n",
    "    )\n",
    "\n",
    "# Make sure that the randomly generated parameters work fine\n",
    "x = torch.randn((50, in_channels, 64, 64)).requires_grad_()\n",
    "conv_gen = TransposeConv2d(\n",
    "    in_channels, out_channels, **settings,\n",
    "    params=None,\n",
    "    )\n",
    "y_ = conv_gen(x)\n",
    "_ = conv_gen.backward(grad=torch.ones_like(y_))\n",
    "\n",
    "# Build the moduels and assert the parameters are identical\n",
    "conv = nn.ConvTranspose2d(\n",
    "    in_channels, out_channels, **settings\n",
    "    )\n",
    "conv_ = TransposeConv2d(\n",
    "    in_channels, out_channels, **settings,\n",
    "    params=dict(weight=conv.weight.clone(), bias=conv.bias.clone())\n",
    "    )\n",
    "torch.testing.assert_allclose(conv_.param()[0][0], conv.weight)\n",
    "torch.testing.assert_allclose(conv_.param()[1][0], conv.bias)\n",
    "\n",
    "# Check the forward and backward methods\n",
    "x = torch.randn((50, in_channels, 64, 64)).requires_grad_()\n",
    "y, y_ = conv(x), conv_(x)\n",
    "torch.testing.assert_allclose(y_, y)\n",
    "torch.testing.assert_allclose(\n",
    "        conv_.backward(grad=torch.ones_like(y_)),\n",
    "        torch.autograd.grad(conv(x), x, grad_outputs=torch.ones_like(y))[0],\n",
    "    )\n",
    "\n",
    "# Check the parameter gradients\n",
    "gradw, = torch.autograd.grad(conv(x), conv.weight, grad_outputs=torch.ones_like(y))\n",
    "gradb, = torch.autograd.grad(conv(x), conv.bias, grad_outputs=torch.ones_like(y))\n",
    "_, gradw_ = conv_.param()[0]\n",
    "_, gradb_ = conv_.param()[1]\n",
    "torch.testing.assert_allclose(gradw_, gradw)\n",
    "torch.testing.assert_allclose(gradb_, gradb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sequential CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((50, 3, 28, 28)).requires_grad_()\n",
    "\n",
    "# Build the convolutional layers\n",
    "conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "conv1_ = Conv2d(in_channels=3, out_channels=32, kernel_size=3,\n",
    "    params=dict(weight=conv1.weight.clone(), bias=conv1.bias.clone()))\n",
    "conv2 = nn.Conv2d(in_channels=32, out_channels=8, kernel_size=5)\n",
    "conv2_ = Conv2d(in_channels=32, out_channels=8, kernel_size=5,\n",
    "    params=dict(weight=conv2.weight.clone(), bias=conv2.bias.clone()))\n",
    "\n",
    "# Build the tansposed convolutional layers\n",
    "tconv2 = nn.ConvTranspose2d(in_channels=8, out_channels=32, kernel_size=5)\n",
    "tconv2_ = TransposeConv2d(in_channels=8, out_channels=32, kernel_size=5,\n",
    "    params=dict(weight=tconv2.weight.clone(), bias=tconv2.bias.clone()))\n",
    "tconv1 = nn.ConvTranspose2d(in_channels=32, out_channels=3, kernel_size=3)\n",
    "tconv1_ = TransposeConv2d(in_channels=32, out_channels=3, kernel_size=3,\n",
    "    params=dict(weight=tconv1.weight.clone(), bias=tconv1.bias.clone()))\n",
    "\n",
    "# Build the sequences\n",
    "seq = nn.Sequential(\n",
    "    conv1,\n",
    "    nn.ReLU(),\n",
    "    conv2,\n",
    "    nn.ReLU(),\n",
    "    tconv2,\n",
    "    nn.ReLU(),\n",
    "    tconv1,\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "seq_ = Sequential(\n",
    "    conv1_,\n",
    "    ReLU(),\n",
    "    conv2_,\n",
    "    ReLU(),\n",
    "    tconv2_,\n",
    "    ReLU(),\n",
    "    tconv1_,\n",
    "    Sigmoid(),\n",
    ")\n",
    "\n",
    "# Test the forward and backward methods\n",
    "y, y_ = seq(x), seq_(x)\n",
    "torch.testing.assert_allclose(y_, y)\n",
    "torch.testing.assert_allclose(\n",
    "        seq_.backward(grad=torch.ones_like(y_)),\n",
    "        torch.autograd.grad(seq(x), x, grad_outputs=torch.ones_like(y))[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test gradient wrt the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((50, 3, 28, 28)).requires_grad_()\n",
    "y = torch.randn((50, 3, 28, 28))\n",
    "\n",
    "# Define the loss functions\n",
    "loss = nn.MSELoss()\n",
    "loss_ = MSE()\n",
    "\n",
    "# Check the forward method\n",
    "torch.testing.assert_allclose(loss_(seq(x), y), loss(seq_(x), y),)\n",
    "\n",
    "# Calculate the gradients wrt to the parameters (Calling the backward methods)\n",
    "seq.zero_grad()\n",
    "loss(seq_(x), y).backward()\n",
    "seq_.zero_grad()\n",
    "grad_inp, grad_tar = loss_.backward()\n",
    "_ = seq_.backward(grad=grad_inp)\n",
    "\n",
    "# Test the gradients wrt the parameters\n",
    "rtol, atol = 1e-03, 5e-03\n",
    "torch.testing.assert_allclose(conv1.weight.grad, conv1_.param()[0][1])\n",
    "torch.testing.assert_allclose(conv1.bias.grad, conv1_.param()[1][1])\n",
    "torch.testing.assert_allclose(conv2.weight.grad, conv2_.param()[0][1])\n",
    "torch.testing.assert_allclose(conv2.bias.grad, conv2_.param()[1][1])\n",
    "torch.testing.assert_allclose(tconv1.weight.grad, tconv1_.param()[0][1], rtol=rtol, atol=atol)\n",
    "torch.testing.assert_allclose(tconv1.bias.grad, tconv1_.param()[1][1], rtol=rtol, atol=atol)\n",
    "torch.testing.assert_allclose(tconv2.weight.grad, tconv2_.param()[0][1], rtol=rtol, atol=atol)\n",
    "torch.testing.assert_allclose(tconv2.bias.grad, tconv2_.param()[1][1], rtol=rtol, atol=atol)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5832cd5bc0ec05c403eb2c1e612446d4cc3407633b4985846c7e31b75857e1e9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('DLC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
